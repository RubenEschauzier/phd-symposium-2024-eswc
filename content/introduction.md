## Introduction
{:#introduction}
The original vision for the web envisioned a decentralized system where everyone can contribute and access information. 
However, currently, data is stored in centralized silos controlled by massive companies like Facebook, Google, and Amazon. 
These companies now control the data generated by web users, restricting innovation [](cite:cites Verborgh_2020).
Various [decentralization efforts](cite:cites decentralizednanopubs, mastodon, solid) have emerged to give back control of personal data to the user. 
These efforts commonly revolve around personal decentralized data vaults, where each user gets their own data storage.
As a result, data becomes highly decentralized, spread over possibly millions of sources. 
Initiatives like Solid use the Resource Description Framework (RDF) to store data in a machine-readable format. 
The W3C-recommended declarative language for querying centralized RDF stores is SPARQL. 
Traditional SPARQL query algorithms are designed to query a singular RDF store, which is known beforehand.
This approach is insufficient for decentralized environments, as there are multiple sources, and the number of sources is not known beforehand.

<!-- Originally, LTQP was used to query the web of Linked Open Data, however, this is impractical due to the absence of prior knowledge of the data queried and the potentially infinite size of the data [](cite:cites hartig2012foundations).
However, by limiting the scope to decentralized environments like Solid, the data size becomes restricted, and the engine can make structural assumptions based on the specification of Solid [](cite:cites taelman2023link). -->

[Federated SPARQL query algorithms](cite:cites saleem2018costfed, taelman2018comunica) are built to query a few large sources [](cite:cites dang2023fedshop, qudus2021empirical) known beforehand and do not support fine-grained access control. 
In decentralized environments like Solid, the number of personal data vaults can exceed millions, with each data vault containing a small number of triples. 
Furthermore, the location of each data vault is not guaranteed to be known before query execution, and data vaults store highly personal data, requiring fine-grained access control.
These intrinsic requirements of decentralized environments mean that federated query processing is not sufficient.
Instead, querying over decentralized environments can be done by [Link Traversal-based Query Processing (LTQP)](cite:cites hartig2009executing).
LTQP is an integrated querying approach where the query engine starts with a set of _seed URIs_ and dynamically discovers sources by following hyperlinks discovered in documents of previously dereferenced URIs. 
This approach allows for fine-grained access control, as an LTQP engine will ignore any document it cannot dereference. Furthermore, LTQP requires no prior knowledge of the location of data sources, as this is discovered on the fly.
Despite these advantages, LTQP still suffers from significant limitations, even in restricted decentralized environments [](cite:cites taelman2023link). 
First, discerning relevant from irrelevant data sources during query execution is [difficult](cite:cites hartig2016walking), as most information on the data sources is only available after an expensive HTTP request. 
Second, LTQP algorithms cannot precompute statistical information or indexes of the queried data, as the queried data is unknown beforehand, even if an engine exclusively queries the same data sources. 
Without prior information on the queried data, LTQP often relies on suboptimal query plans, slowing down query execution.


Current literature on LTQP considers each query as a separate event, without considering the usage of shared engine state between query executions. 
However, LTQP is a client-side query approach where an engine instance exclusively services a single client.
As seen in, for example, [browser usage](cite:cites adar2008large), clients exhibit patterns when using applications or browsing the internet. 
These patterns translate to observable patterns in the queries issued to the query engine.
For example, users might primarily use a singular application, which only requires a subset of all data available in the decentralized ecosystem. 
Additionally, users form [sub-communities](cite:cites ferrara2012large) within applications. 
These sub-communities can induce sub-graphs of data in the decentralized environments that are more frequently accessed by members of the sub-community. 
Query engines need to quantify these patterns and leverage them for significantly improved query optimization. 
When an engine has already seen a large portion of the data in previous queries, it can use previously computed answers, statistics, and indexes to significantly improve query performance. 
To address this gap in research, **I will reformulate the query optimization problem from singular queries to a sequence of (possibly) correlated queries.**
In this formulation, query optimizers can exploit the statistical properties of the sequence to improve the average optimization performance of the sequence. 
Thus, engines become personalized, due to each client-specific engine performing different optimization steps based on patterns exhibited by the queries of the client.


Following this introduction,  [](#LiteratureReview) will eloborate on problem-relevant optimization techniques, which will be used to define the problem statement in [](#ProblemStatementandContributions). The research methodology and evaluation approaches are outlined in [](#method) and [](#EvaluationPlan) respectively. Finally, preliminary results are described in [](#PreliminaryResults) and a conclusion is given in [](#Conclusion).


<!-- Link Traversal-based Query Optimization executes queries by traversing links found in previously dereferenced data sources. ... This means that a significant portion of the query execution computional burden is placed on the client, not server. Client-side query optimization over unknown sources is difficult due to the absence of extensive statistical information on the queried data, and the possibly diverse types of data sources discovered. Additionally, the amount of linked data on the internet is possibly infinite (cite olaf paper here). However, when we look at decentralized environments that are closed and have information on the structure of the environment available beforehand we can use this for query optimization. Solid is such a n environment, with certain predicates denoting the way data is stored and can be traversed [](cite:cites taelman2023link)(insert more explanation here).





(Data aggregators are another example of client that observe patterns in query behavior. These clients will often aggregate the same type of data during its lifetime. Due to the stability in data queried, aggregators are a prime example of user that can benefit optimizers based on a user profiles. (needs paper)) Not sure about this one -->


<!-- General structure:
    - Show that currently we mainly use endpoints to query (open) data, and that LTQP is sidelined
    - Introduce problem we want to fix: querying linked data with access control policies
    - Introduce LTQP as solution
    - Introduce historical problem of LTQP on linked open data
    - Explain why we CAN use it if we are in a closed decentralized environment of linked data with structural properties known beforehand
    - Introduce client-side computational burden and difficulty optimizing during LTQP
    - Using papers, state that we expect clients to exhibit patterns in their usage of applications, which translates into patterns in query usage
    - Link to recommendation systems
    - Propose to use similar techniques to develop a personalized query optimization engine
    - Finally state next sections -->