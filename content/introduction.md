## Introduction
{:#introduction}
Currently, web user data is stored in centralized silos controlled by massive companies like Facebook, Google, and Amazon. 
These companies control the data generated by web users, restricting innovation [](cite:cites Verborgh_2020).
Rather than adopting a 'vertical' approach where all user data from a single service is stored in one central location, various [decentralization efforts](cite:cites decentralizednanopubs, mastodon, solid), advocate for a 'horizontal' approach. 
This approach disperses data, resulting in many smaller data entities. 
For instance, all data about one individual would be stored in a singular location.
As a result, data becomes highly decentralized, spread over possibly millions of sources. 
Decentralized querying approaches must support cross-source queries, as, for example, social media applications frequently aggregate content from various users to construct a homepage.
Initiatives like Solid use the Resource Description Framework (RDF) to store data in a machine-readable format. 
Traditional SPARQL query algorithms are designed to query a singular RDF store, which is known beforehand.
This approach is insufficient for decentralized environments, as there are multiple sources, and the number of sources is not known beforehand.

[Federated SPARQL query algorithms](cite:cites saleem2018costfed, taelman2018comunica) are built to query a few large sources [](cite:cites dang2023fedshop, qudus2021empirical) known ahead of time and do not support fine-grained access control. 
Given that decentralized environments comprise numerous small sources, each housing highly personal data necessitating access control, federated query processing is insufficient.
Instead, querying over decentralized environments can be done by [Link Traversal-based Query Processing (LTQP)](cite:cites hartig2009executing).
LTQP is an integrated querying approach where the query engine starts with a set of _seed URIs_ and dynamically discovers sources by following hyperlinks discovered in documents of previously dereferenced URIs. 
This approach allows for fine-grained access control, as an LTQP engine can ignore any document it cannot dereference and continue traversing to new documents. 
Furthermore, LTQP requires no prior knowledge of the location of data sources, as this is discovered on the fly.
Despite these advantages, LTQP still suffers from significant limitations, even in restricted decentralized environments [](cite:cites taelman2023link). 
First, discerning relevant from irrelevant data sources during query execution is [difficult](cite:cites hartig2016walking), as most information on the data sources is only available after an expensive HTTP request. 
Second, LTQP algorithms cannot precompute statistical information or indexes of the queried data, as the queried data is unknown beforehand.
Without prior information on the queried data, LTQP often relies on suboptimal query plans, slowing down query execution.


Current literature on LTQP considers each query as a separate event, without considering the usage of shared engine state between query executions. 
However, LTQP is a client-side query approach where an engine instance exclusively services a single client.
As seen in, for example, [browser usage](cite:cites adar2008large), clients exhibit patterns when using applications or browsing the internet. 
These patterns translate to observable patterns in the queries issued to the query engine.
For example, users might primarily use a singular application, which only requires a subset of all data available in the decentralized ecosystem. 
Additionally, users form [sub-communities](cite:cites ferrara2012large) within applications. 
These sub-communities can induce sub-graphs of data that are more frequently accessed by members of the sub-community. 
Query engines need to quantify these patterns and leverage them for significantly improved query optimization. 
When an engine has already seen a large portion of the data in previous queries, it can use previously computed answers, statistics, and indexes to significantly improve query performance. 
To address this gap in research, _I will reformulate the query optimization problem from singular queries to a sequence of (possibly) correlated queries._
In this formulation, query optimizers can exploit the statistical properties of the sequence to improve the average optimization performance of the sequence. 
Thus, engines become personalized, due to each client-specific engine performing different optimization steps based on patterns exhibited by the queries of the client.


Following this introduction,  [](#LiteratureReview) will elaborate on problem-relevant optimization techniques, which will be used to define the problem statement in [](#ProblemStatementandContributions). The research methodology and evaluation approaches are outlined in [](#method) and [](#EvaluationPlan) respectively. Finally, preliminary results are described in [](#PreliminaryResults), and a conclusion is given in [](#Conclusion).
